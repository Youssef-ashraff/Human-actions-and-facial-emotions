{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aaa0aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading Models... Please wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Warming up models...\n",
      "WARNING:tensorflow:6 out of the last 50 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000165CAFA62A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 50 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000165CAFA62A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Models Ready!\n",
      "\n",
      "-----------------------------------\n",
      " SYSTEM INPUT SELECTION\n",
      "-----------------------------------\n",
      "1. Live Webcam\n",
      "2. Video File\n",
      "\n",
      " System Started. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import tkinter as tk \n",
    "from tkinter import filedialog\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model \n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as preprocess_emotion \n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as preprocess_action \n",
    "\n",
    "# --- 1. SETTINGS & COLORS ---\n",
    "EMOTION_MODEL_PATH = 'Emotions_best2.h5'\n",
    "ACTION_MODEL_PATH = 'final_sgd_model.h5'\n",
    "\n",
    "IMG_SIZE_EMOTION = (112, 112)\n",
    "IMG_SIZE_ACTION = (112, 112)\n",
    "SEQUENCE_LENGTH = 16 \n",
    "ACTION_THRESHOLD = 0.50  # Confidence threshold\n",
    "\n",
    "# UI Colors\n",
    "COLOR_GREEN  = (0, 255, 0)    \n",
    "COLOR_CYAN   = (255, 255, 0)  \n",
    "COLOR_BLUE   = (255, 0, 0)    \n",
    "COLOR_WHITE  = (255, 255, 255)\n",
    "COLOR_BLACK  = (0, 0, 0)      \n",
    "\n",
    "# --- 2. LOAD MODELS & WARM UP ---\n",
    "print(\" Loading Models... Please wait.\", flush=True)\n",
    "emotion_model = load_model(EMOTION_MODEL_PATH)\n",
    "action_model = load_model(ACTION_MODEL_PATH)\n",
    "face_classifier = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "print(\" Warming up models...\", flush=True)\n",
    "dummy_action = np.zeros((1, 16, 112, 112, 3), dtype='float32')\n",
    "dummy_emotion = np.zeros((1, 112, 112, 1), dtype='float32')\n",
    "action_model.predict(dummy_action, verbose=0)\n",
    "emotion_model.predict(dummy_emotion, verbose=0)\n",
    "print(\" Models Ready!\", flush=True)\n",
    "\n",
    "# Labels\n",
    "EMOTION_LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "ACTION_LABELS = sorted(['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'HammerThrow', 'Hammering', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpRope', 'JumpingJack', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo'])\n",
    "\n",
    "# --- 3. INPUT SELECTION ---\n",
    "print(\"\\n-----------------------------------\", flush=True)\n",
    "print(\" SYSTEM INPUT SELECTION\", flush=True)\n",
    "print(\"-----------------------------------\", flush=True)\n",
    "print(\"1. Live Webcam\", flush=True)\n",
    "print(\"2. Video File\", flush=True)\n",
    "\n",
    "choice = input(\"Enter choice (1 or 2): \")\n",
    "\n",
    "video_source = 0 # Default to Webcam\n",
    "\n",
    "if choice == '2':\n",
    "    print(\" Opening file selector...\", flush=True)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw() \n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select Video File\",\n",
    "        filetypes=[(\"Video Files\", \"*.mp4 *.avi *.mov *.mkv\")]\n",
    "    )\n",
    "    \n",
    "    if file_path:\n",
    "        video_source = file_path\n",
    "        print(f\" Selected: {os.path.basename(file_path)}\", flush=True)\n",
    "    else:\n",
    "        print(\" No file selected. Reverting to Webcam.\", flush=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "\n",
    "# --- 4. MAIN LOOP ---\n",
    "frames_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
    "current_action = \"Neutral\"\n",
    "current_emotion = \"Neutral\"\n",
    "\n",
    "prev_frame_time = 0\n",
    "frame_count = 0\n",
    "\n",
    "print(\"\\n System Started. Press 'q' to quit.\", flush=True)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        if isinstance(video_source, str): \n",
    "            print(\" Replaying video...\", flush=True)\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # --- ACTION RECOGNITION ---\n",
    "    resized_frame = cv2.resize(frame, IMG_SIZE_ACTION)\n",
    "    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "    processed_frame = preprocess_action(rgb_frame.astype('float32'))\n",
    "    frames_queue.append(processed_frame)\n",
    "    \n",
    "    if len(frames_queue) == SEQUENCE_LENGTH and frame_count % 4 == 0:\n",
    "        input_sequence = np.expand_dims(np.array(frames_queue), axis=0)\n",
    "        preds = action_model.predict(input_sequence, verbose=0)[0]\n",
    "        \n",
    "        if np.max(preds) > ACTION_THRESHOLD:\n",
    "            current_action = ACTION_LABELS[np.argmax(preds)]\n",
    "        else:\n",
    "            current_action = \"Neutral\"\n",
    "\n",
    "    # --- EMOTION RECOGNITION ---\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), COLOR_BLUE, 3)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        \n",
    "        try:\n",
    "            roi_gray = cv2.resize(roi_gray, IMG_SIZE_EMOTION, interpolation=cv2.INTER_AREA)\n",
    "            if np.sum([roi_gray]) != 0:\n",
    "                roi = roi_gray.astype('float32')\n",
    "                roi = np.expand_dims(roi, axis=-1)\n",
    "                roi = roi / 127.5 - 1.0 \n",
    "                roi = np.expand_dims(roi, axis=0)\n",
    "                \n",
    "                e_preds = emotion_model.predict(roi, verbose=0)[0]\n",
    "                current_emotion = EMOTION_LABELS[np.argmax(e_preds)]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # --- UI DRAWING ---\n",
    "    new_frame_time = time.time()\n",
    "    fps = 1 / (new_frame_time - prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "    \n",
    "    cv2.rectangle(frame, (0, 0), (width, 85), COLOR_BLACK, -1)\n",
    "    \n",
    "    cv2.putText(frame, f\"ACTION: {current_action}\", (10, 35), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, COLOR_GREEN, 2)\n",
    "    cv2.putText(frame, f\"EMOTION: {current_emotion}\", (10, 75), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, COLOR_CYAN, 2)\n",
    "    \"\"\"\n",
    "    fps_text = f\"FPS: {int(fps)}\"\n",
    "    (text_w, text_h), _ = cv2.getTextSize(fps_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "    cv2.putText(frame, fps_text, (width - text_w - 20, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, COLOR_WHITE, 2)\n",
    "\"\"\"\n",
    "    cv2.imshow('Integrated model', frame)\n",
    "    \n",
    "    frame_count += 1\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311302d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b77c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
